---
title: "ðŸ§  TruthTorchLM: A Comprehensive Library for Truthfulness Assessment in LLMs"
collection: projects
permalink: /projects/2025-05-02-TruthTorchLM/
excerpt: 'An open-source toolkit with 30+ truth methods for evaluating and predicting the truthfulness of LLM outputs.'
date: 2025-05-02
---

<p align="center">
  <img align="center" src="https://github.com/Ybakman/TruthTorchLM/blob/main/ttlm_logo.png?raw=true" width="460px" />
</p>

[ðŸ”— GitHub Repository](https://github.com/Ybakman/TruthTorchLM)

---

ðŸ”¥ **Introducing TruthTorchLM** â€“ an open-source Python library designed to **assess and predict the truthfulness of LLM-generated outputs** across both short-form and long-form content. Developed over the course of a year, TruthTorchLM brings together 30+ cutting-edge methods proposed in recent literature for truthfulness assessment and uncertainty quantification.

---

## ðŸš€ What TruthTorchLM Offers

- **30+ Truth Methods**: Includes methods like **Google search check**, **uncertainty-based scores**, **self-detection**, and **multi-LLM collaboration**.
- **Seamless Integration**: Easily integrates with **Huggingface** and **LiteLLM** with minimal code changes.
- **Evaluation Tools**: Built-in functions to benchmark methods using **AUROC**, **AUPRC**, **PRR**, and **Accuracy**.
- **Calibration Functions**: Normalize output scores for meaningful comparison across methods.
- **Long-Form Truthfulness**: Automatically decomposes long responses into factual claims and assesses their correctness.
- **Extendability**: Easily plug in your own truth methods or evaluation pipelines.


